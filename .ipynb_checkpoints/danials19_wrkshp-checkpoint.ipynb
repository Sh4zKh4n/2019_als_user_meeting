{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io  # skimage's I/O submodule.\n",
    "from skimage import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('bead_pack.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/dani-lbnl/imagexd19/blob/master/dip/data/bead_pack.tif?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv bead_pack.tif?raw=true bead_pack.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls *.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = io.imread('bead_pack.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic image summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('* Shape: {}'.format(img.shape))\n",
    "print('* Type: {}'.format(img.dtype))\n",
    "print('* Range: {}, {}'.format(img.min(), img.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "def slicer(z):\n",
    "    plt.imshow(img[z,:,:], cmap='gray')\n",
    "\n",
    "interact(slicer, z=widgets.IntSlider(min=0,max=60,step=1,value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [skimage.exposure](https://scikit-image.org/docs/stable/api/skimage.exposure.html) - evaluating or changing the exposure of an image<a id='exposure'></a>\n",
    "\n",
    "This module contains a number of functions for adjusting image contrast. We will use `exposure.adjust_gamma`, which performs gamma correction in the input image.\n",
    "\n",
    "\n",
    "[Gamma correction](https://en.wikipedia.org/wiki/Gamma_correction), also known as Power Law Transform, brightens or darkens an image. The function $O = I^\\gamma$ is applied to each pixel in the image. A `gamma < 1` will brighten an image, while a `gamma > 1` will darken an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure\n",
    "ex = exposure.equalize_hist(img)\n",
    "def slicer(z):\n",
    "    plt.imshow(ex[z,:,:], cmap='gray')\n",
    "\n",
    "interact(slicer, z=widgets.IntSlider(min=0,max=60,step=1,value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge detection\n",
    "\n",
    "[Edge detection](https://en.wikipedia.org/wiki/Edge_detection) highlights regions in the image where a sharp change in contrast occurs. The intensity of an edge corresponds to the steepness of the transition from one intensity to another. A gradual shift from bright to dark intensity results in a dim edge. An abrupt shift results in a bright edge.\n",
    "\n",
    "## [skimage.filters](https://scikit-image.org/docs/stable/api/skimage.filters.html) - apply filters to an image<a id='filters'></a>\n",
    "\n",
    "Filtering applies whole-image modifications such as sharpening or blurring. In addition to edge detection, `skimage.filters` provides functions for filtering and thresholding images.\n",
    "\n",
    "Notable functions include (links to relevant gallery examples):\n",
    "\n",
    "* [Thresholding](https://scikit-image.org/docs/stable/auto_examples/applications/plot_thresholding.html):\n",
    "  * `filters.threshold_*` (multiple different functions with this prefix)\n",
    "  * `filters.try_all_threshold` to compare various methods\n",
    "* [Edge finding/enhancement](https://scikit-image.org/docs/stable/auto_examples/edges/plot_edge_filter.html):\n",
    "  * `filters.sobel` - not adapted for 3D images. It can be applied planewise to approximate a 3D result.\n",
    "  * `filters.prewitt`\n",
    "  * `filters.scharr`\n",
    "  * `filters.roberts`\n",
    "  * `filters.laplace`\n",
    "  * `filters.hessian`\n",
    "* [Ridge filters](https://scikit-image.org/docs/stable/auto_examples/edges/plot_ridge_filter.html):\n",
    "  * `filters.meijering`\n",
    "  * `filters.sato`\n",
    "  * `filters.frangi`\n",
    "* Inverse filtering (see also [skimage.restoration](#restoration)):\n",
    "  * `filters.weiner`\n",
    "  * `filters.inverse`\n",
    "* [Directional](https://scikit-image.org/docs/stable/auto_examples/features_detection/plot_gabor.html): `filters.gabor`\n",
    "* Blurring/denoising\n",
    "  * `filters.gaussian`\n",
    "  * `filters.median`\n",
    "* [Sharpening](https://scikit-image.org/docs/stable/auto_examples/filters/plot_unsharp_mask.html): `filters.unsharp_mask`\n",
    "* Define your own filter: `LPIFilter2D`\n",
    "  \n",
    "The sub-submodule `skimage.filters.rank` contains rank filters. These filters are nonlinear and operate on the local histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "img2 = filters.laplace(img)\n",
    "def slicer(z):\n",
    "    plt.imshow(img2[z,:,:], cmap='gray')\n",
    "\n",
    "interact(slicer, z=widgets.IntSlider(min=0,max=60,step=1,value=5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "n=27\n",
    "aslice = img[n,:,:]\n",
    "filters.try_all_threshold(aslice,figsize=(16,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "img3 = filters.gaussian(img)\n",
    "t=filters.threshold_isodata(img3)\n",
    "img3 = img3>t\n",
    "\n",
    "def slicer(z):\n",
    "    plt.imshow(img3[z,:,:], cmap='gray')\n",
    "\n",
    "interact(slicer, z=widgets.IntSlider(min=0,max=60,step=1,value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='morphology'></a>[skimage.morphology](https://scikit-image.org/docs/stable/api/skimage.morphology.html) - binary and grayscale morphology\n",
    "\n",
    "Morphological image processing is a collection of non-linear operations related to the shape or morphology of features in an image, such as boundaries, skeletons, etc. In any given technique, we probe an image with a small shape or template called a structuring element, which defines the region of interest or neighborhood around a pixel.\n",
    "\n",
    "[Mathematical morphology](https://en.wikipedia.org/wiki/Mathematical_morphology) operations and structuring elements are defined in `skimage.morphology`. Structuring elements are shapes which define areas over which an operation is applied. The response to the filter indicates how well the neighborhood corresponds to the structuring element's shape.\n",
    "\n",
    "There are a number of two and three dimensional structuring elements defined in `skimage.morphology`. Not all 2D structuring element have a 3D counterpart. The simplest and most commonly used structuring elements are the `disk`/`ball` and `square`/`cube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology  # skimage's morphological submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball = morphology.ball(radius=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2te=morphology.binary_erosion(img3,selem=ball)\n",
    "img2to=morphology.binary_opening(img3,selem=ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show two 2D images side by side for quick comparison\n",
    "def imshowcmp(before,after,lut):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(20, 20))\n",
    "    ax[0].imshow(before,cmap=lut)\n",
    "    ax[1].imshow(after,cmap=lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshowcmp(img2te[n,:,:],img2to[n,:,:],'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear that we need to improve the segmentation before assigning different labels to disjoint regions as believed in the original sample.\n",
    "\n",
    "Watershed segmentation can distinguish touching objects. Markers are placed at local minima and expanded outward until there is a collision with markers from another region. The inverse intensity image transforms bright cell regions into basins which should be filled.\n",
    "\n",
    "In declumping, markers are generated from the distance function. Points furthest from an edge have the highest intensity and should be identified as markers using skimage.feature.peak_local_max. Regions with pinch points should be assigned multiple markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary=morphology.binary_erosion(img2to,selem=ball)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = binary\n",
    "def slicer(z):\n",
    "    plt.imshow(a[z,:,:], cmap='gray')\n",
    "interact(slicer, z=widgets.IntSlider(min=0,max=60,step=1,value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised classification - flood fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation as seg\n",
    "b = filters.median(img[n,:,:])\n",
    "plt.imshow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_point = (10, 10)  # Experiment with the seed point\n",
    "c = seg.flood(b, seed_point, tolerance=50)\n",
    "plt.imshow(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised - kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "kmeans_cluster = cluster.KMeans(n_clusters=50)\n",
    "kmeans_cluster.fit(b)\n",
    "cluster_centers = kmeans_cluster.cluster_centers_\n",
    "cluster_labels = kmeans_cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cluster_centers[cluster_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itk\n",
    "from itkwidgets import view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_itk = itk.GetImageFromArray(binary.astype(np.uint8))\n",
    "view(image_itk, cmap='Cold and Hot', slicing_planes=True,gradient_opacity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import feature, measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = measure.label(binary)\n",
    "regions = measure.regionprops(label,intensity_image=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(label) #len(regionprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_props = {p:regions[0][p] for p in regions[0] if p not in ('image','convex_image','filled_image')}\n",
    "for p in regions:\n",
    "    print(p.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_itk = itk.GetImageFromArray(label.astype(np.uint16))\n",
    "view(image_itk, cmap='Cold and Hot', slicing_planes=True,gradient_opacity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about some deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inception v3 is a widely-used image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years.\n",
    "- Inception-v3 is a convolutional neural network that is trained on more than a million images from the ImageNet database. The network is **48** layers deep and can classify images into **1000 object** categories, such as keyboard, mouse, pencil, and many animals.\n",
    "- There are a total of 1,281,167 images for training. The number of images for each synset (category) ranges from 732 to 1300. There are 50,000 validation images, with 50 images per synset. There are 100,000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = InceptionV3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is it so fast?** We used transfer learning, a machine learning method which utilizes a pre-trained neural network. For example, the image recognition model called Inception-v3 consists of two parts: \n",
    "- Feature extraction part with a convolutional neural network;\n",
    "- Classification part with fully-connected and softmax layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import transform\n",
    "\n",
    "def inception_predict(image):\n",
    "    # Rescale image to 299x299, as required by InceptionV3\n",
    "    image_prep = transform.resize(image, (299, 299, 3), mode='reflect')\n",
    "    \n",
    "    # Scale image values to [-1, 1], as required by InceptionV3\n",
    "    image_prep = (img_as_float(image_prep) - 0.5) * 2\n",
    "    \n",
    "    predictions = decode_predictions(\n",
    "        net.predict(image_prep[None, ...])\n",
    "    )\n",
    "    \n",
    "    plt.imshow(image, cmap='gray')\n",
    "    \n",
    "    for pred in predictions[0]:\n",
    "        (n, klass, prob) = pred\n",
    "        print(f'{klass:>15} ({prob:.3f})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, img_as_float\n",
    "inception_predict(data.chelsea())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_predict(data.camera())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
